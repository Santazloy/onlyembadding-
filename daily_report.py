# daily_report.py

import logging
import asyncio
import textwrap
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from collections import defaultdict, Counter
from aiogram import Bot
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

from config import GROUP_IDS
from db import (
    get_messages_for_period,
    get_embeddings_for_period,
    get_user_statistics,
    get_group_statistics,
    get_message_clusters
)
from openai_utils import generate_analysis_text, get_embedding

logger = logging.getLogger(__name__)


class AdvancedAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""

    def __init__(self):
        self.similarity_threshold = 0.75
        self.cluster_count = 5

    async def analyze_semantic_clusters(self, embeddings: List[Dict]) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏"""
        if len(embeddings) < self.cluster_count:
            return {"clusters": [], "error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã
        vectors = np.array([emb['embedding_vector'] for emb in embeddings])
        user_ids = [emb['user_id'] for emb in embeddings]

        # K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        kmeans = KMeans(n_clusters=min(self.cluster_count, len(vectors)), random_state=42)
        clusters = kmeans.fit_predict(vectors)

        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_analysis = []
        for i in range(kmeans.n_clusters):
            cluster_indices = np.where(clusters == i)[0]
            cluster_users = [user_ids[idx] for idx in cluster_indices]

            # –¶–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞
            centroid = kmeans.cluster_centers_[i]

            cluster_analysis.append({
                "cluster_id": i,
                "size": len(cluster_indices),
                "dominant_users": Counter(cluster_users).most_common(3),
                "centroid": centroid.tolist()[:10]  # –ü–µ—Ä–≤—ã–µ 10 –∏–∑–º–µ—Ä–µ–Ω–∏–π –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
            })

        return {"clusters": cluster_analysis, "total_messages": len(embeddings)}

    async def analyze_user_evolution(self, user_embeddings: Dict[int, List]) -> Dict[int, Dict]:
        """–ê–Ω–∞–ª–∏–∑ —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–µ–º –∏ —Å—Ç–∏–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        evolution_analysis = {}

        for user_id, embeddings in user_embeddings.items():
            if len(embeddings) < 2:
                continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            embeddings.sort(key=lambda x: x['created_at'])

            # –°—á–∏—Ç–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–µ–π—Ñ
            semantic_drift = []
            for i in range(1, len(embeddings)):
                prev_vec = np.array(embeddings[i - 1]['embedding_vector'])
                curr_vec = np.array(embeddings[i]['embedding_vector'])
                similarity = cosine_similarity([prev_vec], [curr_vec])[0][0]
                semantic_drift.append(1 - similarity)

            # –ê–Ω–∞–ª–∏–∑ –ø–æ—Å—Ç–æ—è–Ω—Å—Ç–≤–∞ —Ç–µ–º
            all_vectors = [np.array(e['embedding_vector']) for e in embeddings]
            avg_similarity = np.mean(cosine_similarity(all_vectors))

            evolution_analysis[user_id] = {
                "semantic_drift": np.mean(semantic_drift) if semantic_drift else 0,
                "topic_consistency": avg_similarity,
                "message_count": len(embeddings),
                "activity_span": (embeddings[-1]['created_at'] - embeddings[0]['created_at']).total_seconds() / 3600
            }

        return evolution_analysis

    async def detect_conversation_patterns(self, messages: List[Dict], embeddings: List[Dict]) -> Dict:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        timeline = []
        emb_dict = {(e['user_id'], e['created_at']): e['embedding_vector']
                    for e in embeddings}

        for msg in messages:
            key = (msg['user_id'], msg['created_at'])
            if key in emb_dict:
                timeline.append({
                    'time': msg['created_at'],
                    'user_id': msg['user_id'],
                    'text': msg['text'],
                    'embedding': emb_dict[key]
                })

        # –ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤ (–æ—Ç–≤–µ—Ç—ã –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –º–∏–Ω—É—Ç)
        dialogues = []
        for i in range(1, len(timeline)):
            time_diff = (timeline[i]['time'] - timeline[i - 1]['time']).total_seconds()
            if time_diff < 300 and timeline[i]['user_id'] != timeline[i - 1]['user_id']:
                vec1 = np.array(timeline[i - 1]['embedding'])
                vec2 = np.array(timeline[i]['embedding'])
                similarity = cosine_similarity([vec1], [vec2])[0][0]

                dialogues.append({
                    'users': (timeline[i - 1]['user_id'], timeline[i]['user_id']),
                    'similarity': similarity,
                    'time_diff': time_diff
                })

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
        dialogue_stats = {
            'total_dialogues': len(dialogues),
            'avg_response_time': np.mean([d['time_diff'] for d in dialogues]) if dialogues else 0,
            'avg_relevance': np.mean([d['similarity'] for d in dialogues]) if dialogues else 0,
            'high_relevance_count': sum(1 for d in dialogues if d['similarity'] > 0.8)
        }

        return dialogue_stats

    async def calculate_influence_score(self, user_stats: Dict, group_embeddings: List[Dict]) -> Dict[int, float]:
        """–†–∞—Å—á–µ—Ç –≤–ª–∏—è–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –¥–∏—Å–∫—É—Å—Å–∏—é"""
        influence_scores = {}

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        user_embeddings = defaultdict(list)
        for emb in group_embeddings:
            user_embeddings[emb['user_id']].append(emb['embedding_vector'])

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        for user_id, vectors in user_embeddings.items():
            if not vectors:
                continue

            # –°—Ä–µ–¥–Ω–∏–π –≤–µ–∫—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_avg_vector = np.mean(vectors, axis=0)

            # –°—á–∏—Ç–∞–µ–º, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            influence = 0
            influence_count = 0

            for i, emb in enumerate(group_embeddings):
                if emb['user_id'] == user_id:
                    # –ò—â–µ–º –æ—Ç–≤–µ—Ç—ã –¥—Ä—É–≥–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Å–ª–µ–¥—É—é—â–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
                    for j in range(i + 1, min(i + 11, len(group_embeddings))):
                        if group_embeddings[j]['user_id'] != user_id:
                            other_vec = np.array(group_embeddings[j]['embedding_vector'])
                            similarity = cosine_similarity([user_avg_vector], [other_vec])[0][0]
                            influence += similarity
                            influence_count += 1

            influence_scores[user_id] = influence / influence_count if influence_count > 0 else 0

        return influence_scores


async def send_reports_for_all_groups(bot: Bot):
    """
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–æ–º –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 00:00 –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤
    """
    for group_id in GROUP_IDS:
        try:
            await send_daily_report(bot, group_id)
        except Exception as e:
            logger.exception(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞ –¥–ª—è –≥—Ä—É–ø–ø—ã {group_id}: {e}")


async def send_daily_report(bot: Bot, group_id: int):
    """
    –û—Ç—á—ë—Ç –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤)
    """
    await send_period_report(bot, group_id, days=1)


async def send_period_report(bot: Bot, group_id: int, days: int):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤ –∑–∞ —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
    """
    now_utc = datetime.utcnow()
    start_time = now_utc - timedelta(days=days)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞ –¥–ª—è –æ—Ç—á–µ—Ç–∞
    period_names = {
        1: "24 —á–∞—Å–∞",
        3: "72 —á–∞—Å–∞",
        7: "–Ω–µ–¥–µ–ª—é",
        30: "–º–µ—Å—è—Ü"
    }
    period_name = period_names.get(days, f"{days} –¥–Ω–µ–π")

    analyzer = AdvancedAnalyzer()

    # 1) –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    messages = await get_messages_for_period(group_id, start_time, now_utc)
    if not messages:
        await bot.send_message(
            group_id, f"–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {period_name} –≤ —ç—Ç–æ–º —á–∞—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –±—ã–ª–æ.")
        return

    embeddings = await get_embeddings_for_period(group_id, start_time, now_utc)
    user_stats = await get_user_statistics(group_id, start_time, now_utc)
    group_stats = await get_group_statistics(group_id, start_time, now_utc)

    # 2) –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑
    cluster_analysis = await analyzer.analyze_semantic_clusters(embeddings)

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
    user_embeddings = defaultdict(list)
    for emb in embeddings:
        user_embeddings[emb['user_id']].append(emb)

    evolution_analysis = await analyzer.analyze_user_evolution(user_embeddings)
    dialogue_patterns = await analyzer.detect_conversation_patterns(messages, embeddings)
    influence_scores = await analyzer.calculate_influence_score(user_stats, embeddings)

    # 3) –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–µ—Ä–∏–æ–¥–∞
    message_limit = min(100 * days, 500)  # –ë–æ–ª—å—à–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
    chat_text = "\n".join(f"{row['user_name']}: {row['text']}"
                          for row in messages[:message_limit])

    # 4) –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
    period_specific_instructions = {
        1: "–§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö –∏ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–µ.",
        3: "–û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –Ω–µ–¥–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á.",
        7: "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –Ω–µ–¥–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º –∏ –∫–æ–º–∞–Ω–¥–Ω—É—é –¥–∏–Ω–∞–º–∏–∫—É.",
        30: "–°–¥–µ–ª–∞–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã, —ç–≤–æ–ª—é—Ü–∏—è –∫–æ–º–∞–Ω–¥—ã, –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –º–µ—Å—è—á–Ω—ã—Ö —Ü–µ–ª–µ–π."
    }

    specific_instruction = period_specific_instructions.get(days,
                                                            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–Ω–∞–º–∏–∫—É –∑–∞ {days} –¥–Ω–µ–π, –≤—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ —Ç—Ä–µ–Ω–¥—ã.")

    prompt = f"""
–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ —Ä–∞–±–æ—á–µ–≥–æ —á–∞—Ç–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {period_name}.

–ü–ï–†–ò–û–î –ê–ù–ê–õ–ò–ó–ê: {period_name} (—Å {start_time.strftime('%d.%m.%Y')} –ø–æ {now_utc.strftime('%d.%m.%Y')})
–°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –ò–ù–°–¢–†–£–ö–¶–ò–Ø: {specific_instruction}

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:

1. –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ì–†–£–ü–ü–´:
- –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {group_stats.get('total_messages', 0)}
- –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {group_stats.get('active_users', 0)}
- –ü–∏–∫–æ–≤—ã–µ —á–∞—Å—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {group_stats.get('peak_hours', '–Ω/–¥')}
- –°—Ä–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –¥–µ–Ω—å: {group_stats.get('total_messages', 0) / days:.1f}

2. –ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã —Ç–µ–º):
{format_cluster_analysis(cluster_analysis)}

3. –ê–ù–ê–õ–ò–ó –≠–í–û–õ–Æ–¶–ò–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô:
{format_evolution_analysis(evolution_analysis, user_stats)}

4. –ü–ê–¢–¢–ï–†–ù–´ –î–ò–ê–õ–û–ì–û–í:
- –í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤: {dialogue_patterns['total_dialogues']}
- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {dialogue_patterns['avg_response_time']:.1f} —Å–µ–∫
- –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤: {dialogue_patterns['avg_relevance']:.2%}
- –í—ã—Å–æ–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {dialogue_patterns['high_relevance_count']}

5. –ò–ù–î–ï–ö–° –í–õ–ò–Ø–ù–ò–Ø –£–ß–ê–°–¢–ù–ò–ö–û–í:
{format_influence_scores(influence_scores, user_stats)}

–§–†–ê–ì–ú–ï–ù–¢–´ –ü–ï–†–ï–ü–ò–°–ö–ò:
{chat_text}

–ó–ê–î–ê–ù–ò–ï:
–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç –∑–∞ {period_name} —Å–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–µ–≥–∏ <code> –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ <pre> –¥–ª—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è.

<code>üìä EXECUTIVE SUMMARY –∑–∞ {period_name}</code>
<pre>
–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞—Ö–æ–¥–æ–∫ (3-5 –ø—É–Ω–∫—Ç–æ–≤):
- –ì–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã –ø–µ—Ä–∏–æ–¥–∞
- –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è/–ø—Ä–æ–±–ª–µ–º—ã
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏
- –û–±—â–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –∫–æ–º–∞–Ω–¥—ã
{f'- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –ø–µ—Ä–∏–æ–¥–æ–º' if days > 1 else ''}
</pre>

<code>üë• –ò–ù–î–ò–í–ò–î–£–ê–õ–¨–ù–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê</code>
<pre>
–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞ (—Ç–æ–ø-5 –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏):
[–ò–º—è —É—á–∞—Å—Ç–Ω–∏–∫–∞]:
‚Ä¢ –†–æ–ª—å –≤ –∫–æ–º–∞–Ω–¥–µ: (–ª–∏–¥–µ—Ä/–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å/—ç–∫—Å–ø–µ—Ä—Ç/–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä)
‚Ä¢ –í–∫–ª–∞–¥ –∑–∞ –ø–µ—Ä–∏–æ–¥: (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è/—Ä–µ—à–µ–Ω–∏—è)
‚Ä¢ –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: (—á—Ç–æ –¥–µ–ª–∞–µ—Ç —Ö–æ—Ä–æ—à–æ)
‚Ä¢ –ó–æ–Ω—ã —Ä–∞–∑–≤–∏—Ç–∏—è: (—á—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
‚Ä¢ –ò–Ω–¥–µ–∫—Å –≤–ª–∏—è–Ω–∏—è: X.XX (–Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–∏—Å–∫—É—Å—Å–∏—é)
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–µ–π—Ñ: X.XX (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º)
{f'‚Ä¢ –î–∏–Ω–∞–º–∏–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥: (—Ä–æ—Å—Ç/—Å–ø–∞–¥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏)' if days > 1 else ''}
</pre>

<code>üéØ –ê–ù–ê–õ–ò–ó –ü–†–û–î–£–ö–¢–ò–í–ù–û–°–¢–ò</code>
<pre>
‚Ä¢ –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏: (—Å–ø–∏—Å–æ–∫ —Å –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è–º–∏)
‚Ä¢ –ù–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏: (—Å —É–∫–∞–∑–∞–Ω–∏–µ–º –±–ª–æ–∫–µ—Ä–æ–≤)
‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π: X/10
‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π: (–±—ã—Å—Ç—Ä–æ/—Å—Ä–µ–¥–Ω–µ/–º–µ–¥–ª–µ–Ω–Ω–æ)
‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: (–ø—Ä–∏–º–µ—Ä—ã)
{f'‚Ä¢ –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–º —Ü–µ–ª—è–º' if days >= 7 else ''}
</pre>

<code>üîç –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó</code>
<pre>
‚Ä¢ –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã:
  1. [–¢–µ–º–∞] - XX% –¥–∏—Å–∫—É—Å—Å–∏–π
  2. [–¢–µ–º–∞] - XX% –¥–∏—Å–∫—É—Å—Å–∏–π
  3. [–¢–µ–º–∞] - XX% –¥–∏—Å–∫—É—Å—Å–∏–π
‚Ä¢ –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–Ω: (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π/–Ω–∞–ø—Ä—è–∂—ë–Ω–Ω—ã–π)
‚Ä¢ –£—Ä–æ–≤–µ–Ω—å –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏: X/10
{f'‚Ä¢ –≠–≤–æ–ª—é—Ü–∏—è —Ç–µ–º –∑–∞ –ø–µ—Ä–∏–æ–¥' if days > 1 else ''}
</pre>

<code>‚ö†Ô∏è –†–ò–°–ö–ò –ò –ü–†–û–ë–õ–ï–ú–ù–´–ï –ó–û–ù–´</code>
<pre>
‚Ä¢ –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏: (–Ω–µ–¥–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è, –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã)
‚Ä¢ –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏: (—Å—Ä—ã–≤—ã —Å—Ä–æ–∫–æ–≤, –∫–∞—á–µ—Å—Ç–≤–æ)
‚Ä¢ –ö–æ–º–∞–Ω–¥–Ω—ã–µ —Ä–∏—Å–∫–∏: (–≤—ã–≥–æ—Ä–∞–Ω–∏–µ, –¥–µ–º–æ—Ç–∏–≤–∞—Ü–∏—è)
‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ç—Ä–µ—Å—Å–∞ —É: [—Å–ø–∏—Å–æ–∫ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤]
{f'‚Ä¢ –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∑–∞ –ø–µ—Ä–∏–æ–¥' if days >= 7 else ''}
</pre>

<code>üìà –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò</code>
<pre>
{'–ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:' if days == 1 else f'–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ {days} –¥–Ω–µ–π:'}
1. [–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å] - –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π: [–ö—Ç–æ]
2. [–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å] - –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π: [–ö—Ç–æ]
3. [–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å] - –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π: [–ö—Ç–æ]

{'–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:' if days >= 7 else '–¢–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è:'}
‚Ä¢ –ü–æ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ—Ü–µ—Å—Å–æ–≤:
‚Ä¢ –ü–æ —Ä–∞–∑–≤–∏—Ç–∏—é –∫–æ–º–∞–Ω–¥—ã:
‚Ä¢ –ü–æ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è–º:
</pre>

<code>üìä –ú–ï–¢–†–ò–ö–ò –î–õ–Ø –û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–Ø</code>
<pre>
‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—à—ë–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: X
‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ: X —á–∞—Å–æ–≤
‚Ä¢ –ò–Ω–¥–µ–∫—Å –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: X.X/10
‚Ä¢ –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ KPI: XX%
{f'‚Ä¢ –¢—Ä–µ–Ω–¥ –∑–∞ –ø–µ—Ä–∏–æ–¥: ‚Üë‚Üì‚Üí' if days > 1 else ''}
</pre>

–ò—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è –≤—ã–≤–æ–¥–æ–≤. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω, –ø—Ä–∏–≤–æ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã.
–ê–¥–∞–ø—Ç–∏—Ä—É–π –≥–ª—É–±–∏–Ω—É –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–¥ –ø–µ—Ä–∏–æ–¥: {period_name}.
"""

    # 4) –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
    report = await generate_analysis_text(prompt, model="gpt-4o", max_tokens=4000)
    if not report:
        await bot.send_message(group_id,
                               "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç—á—ë—Ç –æ—Ç GPT.")
        return

    # 5) –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    stats_appendix = await format_statistical_appendix(user_stats, group_stats, influence_scores, days)
    full_report = report + "\n\n" + stats_appendix

    # 6) –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    await send_long_html(bot, group_id, full_report)


def format_cluster_analysis(cluster_analysis: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    if 'error' in cluster_analysis:
        return cluster_analysis['error']

    result = []
    for cluster in cluster_analysis.get('clusters', []):
        users_str = ", ".join([f"User_{uid} ({count})"
                               for uid, count in cluster['dominant_users']])
        result.append(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster['cluster_id']}: "
                      f"{cluster['size']} —Å–æ–æ–±—â–µ–Ω–∏–π, "
                      f"–æ—Å–Ω–æ–≤–Ω—ã–µ —É—á–∞—Å—Ç–Ω–∏–∫–∏: {users_str}")

    return "\n".join(result) or "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"


def format_evolution_analysis(evolution: Dict, user_stats: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —ç–≤–æ–ª—é—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    result = []
    for user_id, analysis in evolution.items():
        user_name = user_stats.get(user_id, {}).get('user_name', f'User_{user_id}')
        result.append(
            f"{user_name}: "
            f"–¥—Ä–µ–π—Ñ —Ç–µ–º: {analysis['semantic_drift']:.2f}, "
            f"–ø–æ—Å—Ç–æ—è–Ω—Å—Ç–≤–æ: {analysis['topic_consistency']:.2f}, "
            f"—Å–æ–æ–±—â–µ–Ω–∏–π: {analysis['message_count']}"
        )

    return "\n".join(result[:10]) or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"  # –¢–æ–ø-10


def format_influence_scores(scores: Dict, user_stats: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤–ª–∏—è–Ω–∏—è"""
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    result = []

    for user_id, score in sorted_scores[:10]:  # –¢–æ–ø-10
        user_name = user_stats.get(user_id, {}).get('user_name', f'User_{user_id}')
        result.append(f"{user_name}: {score:.3f}")

    return "\n".join(result) or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"


async def format_statistical_appendix(user_stats: Dict, group_stats: Dict,
                                      influence_scores: Dict, days: int = 1) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    period_name = {1: "24 —á–∞—Å–∞", 3: "72 —á–∞—Å–∞", 7: "–Ω–µ–¥–µ–ª—é", 30: "–º–µ—Å—è—Ü"}.get(days, f"{days} –¥–Ω–µ–π")

    appendix = f"<code>üìà –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–û–ï –ü–†–ò–õ–û–ñ–ï–ù–ò–ï –∑–∞ {period_name}</code>\n<pre>\n"

    # –¢–æ–ø —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º
    appendix += "–¢–û–ü-10 –ü–û –ê–ö–¢–ò–í–ù–û–°–¢–ò:\n"
    sorted_users = sorted(user_stats.items(),
                          key=lambda x: x[1].get('message_count', 0),
                          reverse=True)[:10]

    for i, (user_id, stats) in enumerate(sorted_users, 1):
        msg_count = stats.get('message_count', 0)
        avg_per_day = msg_count / days
        appendix += (f"{i}. {stats.get('user_name', f'User_{user_id}')}: "
                     f"{msg_count} —Å–æ–æ–±—â–µ–Ω–∏–π ({avg_per_day:.1f}/–¥–µ–Ω—å), "
                     f"–≤–ª–∏—è–Ω–∏–µ: {influence_scores.get(user_id, 0):.3f}\n")

    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    appendix += f"\n–ü–ò–ö–û–í–´–ï –ß–ê–°–´: {group_stats.get('peak_hours', '–Ω/–¥')}\n"
    appendix += f"–í–°–ï–ì–û –£–ù–ò–ö–ê–õ–¨–ù–´–• –¢–ï–ú: {group_stats.get('unique_topics', '–Ω/–¥')}\n"

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
    if days >= 7:
        appendix += f"\n–°–†–ï–î–ù–Ø–Ø –ê–ö–¢–ò–í–ù–û–°–¢–¨ –ü–û –î–ù–Ø–ú:\n"
        appendix += f"–†–∞–±–æ—á–∏–µ –¥–Ω–∏: {group_stats.get('weekday_avg', '–Ω/–¥')} —Å–æ–æ–±—â–µ–Ω–∏–π\n"
        appendix += f"–í—ã—Ö–æ–¥–Ω—ã–µ: {group_stats.get('weekend_avg', '–Ω/–¥')} —Å–æ–æ–±—â–µ–Ω–∏–π\n"

    if days >= 30:
        appendix += f"\n–î–ò–ù–ê–ú–ò–ö–ê –ü–û –ù–ï–î–ï–õ–Ø–ú:\n"
        appendix += f"–ù–µ–¥–µ–ª—è 1: {group_stats.get('week1_messages', '–Ω/–¥')} —Å–æ–æ–±—â–µ–Ω–∏–π\n"
        appendix += f"–ù–µ–¥–µ–ª—è 2: {group_stats.get('week2_messages', '–Ω/–¥')} —Å–æ–æ–±—â–µ–Ω–∏–π\n"
        appendix += f"–ù–µ–¥–µ–ª—è 3: {group_stats.get('week3_messages', '–Ω/–¥')} —Å–æ–æ–±—â–µ–Ω–∏–π\n"
        appendix += f"–ù–µ–¥–µ–ª—è 4: {group_stats.get('week4_messages', '–Ω/–¥')} —Å–æ–æ–±—â–µ–Ω–∏–π\n"

    appendix += "</pre>"
    return appendix


def fix_html_tags(text: str) -> str:
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—ã—Ç–∞–µ—Ç—Å—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å <pre>...</pre>.
    """
    text = text.strip()
    opens = text.count("<pre>")
    closes = text.count("</pre>")
    if opens > closes:
        diff = opens - closes
        text += "</pre>" * diff
    elif closes > opens:
        diff = closes - opens
        for _ in range(diff):
            idx = text.rfind("</pre>")
            if idx >= 0:
                text = text[:idx] + text[idx + 6:]
    return text


async def send_long_html(bot: Bot, chat_id: int, text: str, chunk_size=4000):
    """
    –î—Ä–æ–±–∏–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º parse_mode=HTML.
    """
    text = fix_html_tags(text.strip())

    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ü–µ–ª–∏–∫–æ–º
    if len(text) <= chunk_size:
        try:
            await bot.send_message(chat_id, text, parse_mode="HTML")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ HTML: {e}")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
            await bot.send_message(chat_id, text)
        return

    # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    chunks = []
    current_chunk = ""
    lines = text.split('\n')

    for line in lines:
        # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
        if len(current_chunk) + len(line) + 1 > chunk_size:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = line
        else:
            if current_chunk:
                current_chunk += '\n'
            current_chunk += line

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
    if current_chunk:
        chunks.append(current_chunk)

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏
    for i, chunk in enumerate(chunks):
        chunk = fix_html_tags(chunk)
        try:
            await bot.send_message(chat_id, chunk, parse_mode="HTML")
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
            if i < len(chunks) - 1:
                await asyncio.sleep(0.5)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —á–∞–Ω–∫–∞ {i + 1}: {e}")
            # –ü—Ä–æ–±—É–µ–º –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –±–µ–∑ HTML
            await bot.send_message(chat_id, chunk)
