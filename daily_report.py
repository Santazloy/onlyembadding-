# daily_report.py

import logging
import asyncio
import textwrap
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from collections import defaultdict, Counter
from aiogram import Bot
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

from config import GROUP_IDS
from db import (
    get_messages_for_period,
    get_embeddings_for_period,
    get_user_statistics,
    get_group_statistics,
    get_message_clusters
)
from openai_utils import generate_analysis_text, get_embedding

logger = logging.getLogger(__name__)


class AdvancedAnalyzer:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""

    def __init__(self):
        self.similarity_threshold = 0.75
        self.cluster_count = 5

    async def analyze_semantic_clusters(self, embeddings: List[Dict]) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏"""
        if len(embeddings) < self.cluster_count:
            return {"clusters": [], "error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã
        vectors = np.array([emb['embedding_vector'] for emb in embeddings])
        user_ids = [emb['user_id'] for emb in embeddings]

        # K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        kmeans = KMeans(n_clusters=min(self.cluster_count, len(vectors)), random_state=42)
        clusters = kmeans.fit_predict(vectors)

        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_analysis = []
        for i in range(kmeans.n_clusters):
            cluster_indices = np.where(clusters == i)[0]
            cluster_users = [user_ids[idx] for idx in cluster_indices]

            # –¶–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞
            centroid = kmeans.cluster_centers_[i]

            cluster_analysis.append({
                "cluster_id": i,
                "size": len(cluster_indices),
                "dominant_users": Counter(cluster_users).most_common(3),
                "centroid": centroid.tolist()[:10]  # –ü–µ—Ä–≤—ã–µ 10 –∏–∑–º–µ—Ä–µ–Ω–∏–π –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
            })

        return {"clusters": cluster_analysis, "total_messages": len(embeddings)}

    async def analyze_user_evolution(self, user_embeddings: Dict[int, List]) -> Dict[int, Dict]:
        """–ê–Ω–∞–ª–∏–∑ —ç–≤–æ–ª—é—Ü–∏–∏ —Ç–µ–º –∏ —Å—Ç–∏–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        evolution_analysis = {}

        for user_id, embeddings in user_embeddings.items():
            if len(embeddings) < 2:
                continue

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            embeddings.sort(key=lambda x: x['created_at'])

            # –°—á–∏—Ç–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–µ–π—Ñ
            semantic_drift = []
            for i in range(1, len(embeddings)):
                prev_vec = np.array(embeddings[i - 1]['embedding_vector'])
                curr_vec = np.array(embeddings[i]['embedding_vector'])
                similarity = cosine_similarity([prev_vec], [curr_vec])[0][0]
                semantic_drift.append(1 - similarity)

            # –ê–Ω–∞–ª–∏–∑ –ø–æ—Å—Ç–æ—è–Ω—Å—Ç–≤–∞ —Ç–µ–º
            all_vectors = [np.array(e['embedding_vector']) for e in embeddings]
            avg_similarity = np.mean(cosine_similarity(all_vectors))

            evolution_analysis[user_id] = {
                "semantic_drift": np.mean(semantic_drift) if semantic_drift else 0,
                "topic_consistency": avg_similarity,
                "message_count": len(embeddings),
                "activity_span": (embeddings[-1]['created_at'] - embeddings[0]['created_at']).total_seconds() / 3600
            }

        return evolution_analysis

    async def detect_conversation_patterns(self, messages: List[Dict], embeddings: List[Dict]) -> Dict:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        timeline = []
        emb_dict = {(e['user_id'], e['created_at']): e['embedding_vector']
                    for e in embeddings}

        for msg in messages:
            key = (msg['user_id'], msg['created_at'])
            if key in emb_dict:
                timeline.append({
                    'time': msg['created_at'],
                    'user_id': msg['user_id'],
                    'text': msg['text'],
                    'embedding': emb_dict[key]
                })

        # –ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤ (–æ—Ç–≤–µ—Ç—ã –≤ —Ç–µ—á–µ–Ω–∏–µ 5 –º–∏–Ω—É—Ç)
        dialogues = []
        for i in range(1, len(timeline)):
            time_diff = (timeline[i]['time'] - timeline[i - 1]['time']).total_seconds()
            if time_diff < 300 and timeline[i]['user_id'] != timeline[i - 1]['user_id']:
                vec1 = np.array(timeline[i - 1]['embedding'])
                vec2 = np.array(timeline[i]['embedding'])
                similarity = cosine_similarity([vec1], [vec2])[0][0]

                dialogues.append({
                    'users': (timeline[i - 1]['user_id'], timeline[i]['user_id']),
                    'similarity': similarity,
                    'time_diff': time_diff
                })

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
        dialogue_stats = {
            'total_dialogues': len(dialogues),
            'avg_response_time': np.mean([d['time_diff'] for d in dialogues]) if dialogues else 0,
            'avg_relevance': np.mean([d['similarity'] for d in dialogues]) if dialogues else 0,
            'high_relevance_count': sum(1 for d in dialogues if d['similarity'] > 0.8)
        }

        return dialogue_stats

    async def calculate_influence_score(self, user_stats: Dict, group_embeddings: List[Dict]) -> Dict[int, float]:
        """–†–∞—Å—á–µ—Ç –≤–ª–∏—è–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –¥–∏—Å–∫—É—Å—Å–∏—é"""
        influence_scores = {}

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        user_embeddings = defaultdict(list)
        for emb in group_embeddings:
            user_embeddings[emb['user_id']].append(emb['embedding_vector'])

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        for user_id, vectors in user_embeddings.items():
            if not vectors:
                continue

            # –°—Ä–µ–¥–Ω–∏–π –≤–µ–∫—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_avg_vector = np.mean(vectors, axis=0)

            # –°—á–∏—Ç–∞–µ–º, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö –ø–æ—Ö–æ–∂–∏ –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            influence = 0
            influence_count = 0

            for i, emb in enumerate(group_embeddings):
                if emb['user_id'] == user_id:
                    # –ò—â–µ–º –æ—Ç–≤–µ—Ç—ã –¥—Ä—É–≥–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Å–ª–µ–¥—É—é—â–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
                    for j in range(i + 1, min(i + 11, len(group_embeddings))):
                        if group_embeddings[j]['user_id'] != user_id:
                            other_vec = np.array(group_embeddings[j]['embedding_vector'])
                            similarity = cosine_similarity([user_avg_vector], [other_vec])[0][0]
                            influence += similarity
                            influence_count += 1

            influence_scores[user_id] = influence / influence_count if influence_count > 0 else 0

        return influence_scores


async def send_reports_for_all_groups(bot: Bot):
    """
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–æ–º –∫–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 00:00 (–∏–ª–∏ –∫–æ–≥–¥–∞ –≤–∞–º –Ω—É–∂–Ω–æ),
    –∏–ª–∏ –≤—Ä—É—á–Ω—É—é –ø–æ /report –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø.
    """
    for group_id in GROUP_IDS:
        try:
            await send_daily_report(bot, group_id)
        except Exception as e:
            logger.exception(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞ –¥–ª—è –≥—Ä—É–ø–ø—ã {group_id}: {e}")


async def send_daily_report(bot: Bot, group_id: int):
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –æ—Ç—á—ë—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    now_utc = datetime.utcnow()
    day_ago_utc = now_utc - timedelta(days=1)

    analyzer = AdvancedAnalyzer()

    # 1) –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    messages = await get_messages_for_period(group_id, day_ago_utc, now_utc)
    if not messages:
        await bot.send_message(
            group_id, "–ó–∞ –ø—Ä–æ—à–µ–¥—à–∏–π –¥–µ–Ω—å –≤ —ç—Ç–æ–º —á–∞—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –±—ã–ª–æ.")
        return

    embeddings = await get_embeddings_for_period(group_id, day_ago_utc, now_utc)
    user_stats = await get_user_statistics(group_id, day_ago_utc, now_utc)
    group_stats = await get_group_statistics(group_id, day_ago_utc, now_utc)

    # 2) –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑
    cluster_analysis = await analyzer.analyze_semantic_clusters(embeddings)

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
    user_embeddings = defaultdict(list)
    for emb in embeddings:
        user_embeddings[emb['user_id']].append(emb)

    evolution_analysis = await analyzer.analyze_user_evolution(user_embeddings)
    dialogue_patterns = await analyzer.detect_conversation_patterns(messages, embeddings)
    influence_scores = await analyzer.calculate_influence_score(user_stats, embeddings)

    # 3) –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    chat_text = "\n".join(f"{row['user_name']}: {row['text']}"
                          for row in messages[:100])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞

    prompt = f"""
–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ —Ä–∞–±–æ—á–µ–≥–æ —á–∞—Ç–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞.

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:

1. –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ì–†–£–ü–ü–´:
- –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {group_stats.get('total_messages', 0)}
- –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {group_stats.get('active_users', 0)}
- –ü–∏–∫–æ–≤—ã–µ —á–∞—Å—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {group_stats.get('peak_hours', '–Ω/–¥')}

2. –ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã —Ç–µ–º):
{format_cluster_analysis(cluster_analysis)}

3. –ê–ù–ê–õ–ò–ó –≠–í–û–õ–Æ–¶–ò–ò –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô:
{format_evolution_analysis(evolution_analysis, user_stats)}

4. –ü–ê–¢–¢–ï–†–ù–´ –î–ò–ê–õ–û–ì–û–í:
- –í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤: {dialogue_patterns['total_dialogues']}
- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {dialogue_patterns['avg_response_time']:.1f} —Å–µ–∫
- –°—Ä–µ–¥–Ω—è—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤: {dialogue_patterns['avg_relevance']:.2%}
- –í—ã—Å–æ–∫–æ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {dialogue_patterns['high_relevance_count']}

5. –ò–ù–î–ï–ö–° –í–õ–ò–Ø–ù–ò–Ø –£–ß–ê–°–¢–ù–ò–ö–û–í:
{format_influence_scores(influence_scores, user_stats)}

–§–†–ê–ì–ú–ï–ù–¢–´ –ü–ï–†–ï–ü–ò–°–ö–ò:
{chat_text}

–ó–ê–î–ê–ù–ò–ï:
–°–æ–∑–¥–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç —Å–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–µ–≥–∏ <code> –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏ <pre> –¥–ª—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è.

<code>üìä EXECUTIVE SUMMARY</code>
<pre>
–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞—Ö–æ–¥–æ–∫ (3-5 –ø—É–Ω–∫—Ç–æ–≤):
- –ì–ª–∞–≤–Ω—ã–µ —Ç–µ–º—ã –¥–Ω—è
- –ö–ª—é—á–µ–≤—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è/–ø—Ä–æ–±–ª–µ–º—ã
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏
- –û–±—â–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –∫–æ–º–∞–Ω–¥—ã
</pre>

<code>üë• –ò–ù–î–ò–í–ò–î–£–ê–õ–¨–ù–ê–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ê</code>
<pre>
–î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞ (—Ç–æ–ø-5 –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏):
[–ò–º—è —É—á–∞—Å—Ç–Ω–∏–∫–∞]:
‚Ä¢ –†–æ–ª—å –≤ –∫–æ–º–∞–Ω–¥–µ: (–ª–∏–¥–µ—Ä/–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å/—ç–∫—Å–ø–µ—Ä—Ç/–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä)
‚Ä¢ –í–∫–ª–∞–¥ –∑–∞ –¥–µ–Ω—å: (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è/—Ä–µ—à–µ–Ω–∏—è)
‚Ä¢ –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: (—á—Ç–æ –¥–µ–ª–∞–µ—Ç —Ö–æ—Ä–æ—à–æ)
‚Ä¢ –ó–æ–Ω—ã —Ä–∞–∑–≤–∏—Ç–∏—è: (—á—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
‚Ä¢ –ò–Ω–¥–µ–∫—Å –≤–ª–∏—è–Ω–∏—è: X.XX (–Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–∏—Å–∫—É—Å—Å–∏—é)
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–µ–π—Ñ: X.XX (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º)
</pre>

<code>üéØ –ê–ù–ê–õ–ò–ó –ü–†–û–î–£–ö–¢–ò–í–ù–û–°–¢–ò</code>
<pre>
‚Ä¢ –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏: (—Å–ø–∏—Å–æ–∫ —Å –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è–º–∏)
‚Ä¢ –ù–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏: (—Å —É–∫–∞–∑–∞–Ω–∏–µ–º –±–ª–æ–∫–µ—Ä–æ–≤)
‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π: X/10
‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π: (–±—ã—Å—Ç—Ä–æ/—Å—Ä–µ–¥–Ω–µ/–º–µ–¥–ª–µ–Ω–Ω–æ)
‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: (–ø—Ä–∏–º–µ—Ä—ã)
</pre>

<code>üîç –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó</code>
<pre>
‚Ä¢ –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã:
  1. [–¢–µ–º–∞] - XX% –¥–∏—Å–∫—É—Å—Å–∏–π
  2. [–¢–µ–º–∞] - XX% –¥–∏—Å–∫—É—Å—Å–∏–π
  3. [–¢–µ–º–∞] - XX% –¥–∏—Å–∫—É—Å—Å–∏–π
‚Ä¢ –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–Ω: (–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π/–Ω–∞–ø—Ä—è–∂—ë–Ω–Ω—ã–π)
‚Ä¢ –£—Ä–æ–≤–µ–Ω—å –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç–∏: X/10
</pre>

<code>‚ö†Ô∏è –†–ò–°–ö–ò –ò –ü–†–û–ë–õ–ï–ú–ù–´–ï –ó–û–ù–´</code>
<pre>
‚Ä¢ –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏: (–Ω–µ–¥–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è, –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã)
‚Ä¢ –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ä–∏—Å–∫–∏: (—Å—Ä—ã–≤—ã —Å—Ä–æ–∫–æ–≤, –∫–∞—á–µ—Å—Ç–≤–æ)
‚Ä¢ –ö–æ–º–∞–Ω–¥–Ω—ã–µ —Ä–∏—Å–∫–∏: (–≤—ã–≥–æ—Ä–∞–Ω–∏–µ, –¥–µ–º–æ—Ç–∏–≤–∞—Ü–∏—è)
‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å—Ç—Ä–µ—Å—Å–∞ —É: [—Å–ø–∏—Å–æ–∫ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤]
</pre>

<code>üìà –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò</code>
<pre>
–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –∑–∞–≤—Ç—Ä–∞:
1. [–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å] - –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π: [–ö—Ç–æ]
2. [–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å] - –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π: [–ö—Ç–æ]
3. [–ß—Ç–æ —Å–¥–µ–ª–∞—Ç—å] - –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π: [–ö—Ç–æ]

–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
‚Ä¢ –ü–æ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ—Ü–µ—Å—Å–æ–≤:
‚Ä¢ –ü–æ —Ä–∞–∑–≤–∏—Ç–∏—é –∫–æ–º–∞–Ω–¥—ã:
‚Ä¢ –ü–æ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è–º:
</pre>

<code>üìä –ú–ï–¢–†–ò–ö–ò –î–õ–Ø –û–¢–°–õ–ï–ñ–ò–í–ê–ù–ò–Ø</code>
<pre>
‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—à—ë–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: X
‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ: X —á–∞—Å–æ–≤
‚Ä¢ –ò–Ω–¥–µ–∫—Å –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: X.X/10
‚Ä¢ –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–æ KPI: XX%
</pre>

–ò—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è –≤—ã–≤–æ–¥–æ–≤. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω, –ø—Ä–∏–≤–æ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã.
"""

    # 4) –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
    report = await generate_analysis_text(prompt, model="gpt-4o", max_tokens=4000)
    if not report:
        await bot.send_message(group_id,
                               "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç—á—ë—Ç –æ—Ç GPT.")
        return

    # 5) –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    stats_appendix = await format_statistical_appendix(user_stats, group_stats, influence_scores)
    full_report = report + "\n\n" + stats_appendix

    # 6) –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    await send_long_html(bot, group_id, full_report)


def format_cluster_analysis(cluster_analysis: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    if 'error' in cluster_analysis:
        return cluster_analysis['error']

    result = []
    for cluster in cluster_analysis.get('clusters', []):
        users_str = ", ".join([f"User_{uid} ({count})"
                               for uid, count in cluster['dominant_users']])
        result.append(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster['cluster_id']}: "
                      f"{cluster['size']} —Å–æ–æ–±—â–µ–Ω–∏–π, "
                      f"–æ—Å–Ω–æ–≤–Ω—ã–µ —É—á–∞—Å—Ç–Ω–∏–∫–∏: {users_str}")

    return "\n".join(result) or "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"


def format_evolution_analysis(evolution: Dict, user_stats: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —ç–≤–æ–ª—é—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    result = []
    for user_id, analysis in evolution.items():
        user_name = user_stats.get(user_id, {}).get('user_name', f'User_{user_id}')
        result.append(
            f"{user_name}: "
            f"–¥—Ä–µ–π—Ñ —Ç–µ–º: {analysis['semantic_drift']:.2f}, "
            f"–ø–æ—Å—Ç–æ—è–Ω—Å—Ç–≤–æ: {analysis['topic_consistency']:.2f}, "
            f"—Å–æ–æ–±—â–µ–Ω–∏–π: {analysis['message_count']}"
        )

    return "\n".join(result[:10]) or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"  # –¢–æ–ø-10


def format_influence_scores(scores: Dict, user_stats: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤–ª–∏—è–Ω–∏—è"""
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    result = []

    for user_id, score in sorted_scores[:10]:  # –¢–æ–ø-10
        user_name = user_stats.get(user_id, {}).get('user_name', f'User_{user_id}')
        result.append(f"{user_name}: {score:.3f}")

    return "\n".join(result) or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"


async def format_statistical_appendix(user_stats: Dict, group_stats: Dict,
                                      influence_scores: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    appendix = "<code>üìà –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–û–ï –ü–†–ò–õ–û–ñ–ï–ù–ò–ï</code>\n<pre>\n"

    # –¢–æ–ø —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ø–æ —Å–æ–æ–±—â–µ–Ω–∏—è–º
    appendix += "–¢–û–ü-10 –ü–û –ê–ö–¢–ò–í–ù–û–°–¢–ò:\n"
    sorted_users = sorted(user_stats.items(),
                          key=lambda x: x[1].get('message_count', 0),
                          reverse=True)[:10]

    for i, (user_id, stats) in enumerate(sorted_users, 1):
        appendix += (f"{i}. {stats.get('user_name', f'User_{user_id}')}: "
                     f"{stats.get('message_count', 0)} —Å–æ–æ–±—â–µ–Ω–∏–π, "
                     f"–≤–ª–∏—è–Ω–∏–µ: {influence_scores.get(user_id, 0):.3f}\n")

    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    appendix += f"\n–ü–ò–ö–û–í–´–ï –ß–ê–°–´: {group_stats.get('peak_hours', '–Ω/–¥')}\n"
    appendix += f"–í–°–ï–ì–û –£–ù–ò–ö–ê–õ–¨–ù–´–• –¢–ï–ú: {group_stats.get('unique_topics', '–Ω/–¥')}\n"

    appendix += "</pre>"
    return appendix


def fix_html_tags(text: str) -> str:
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—ã—Ç–∞–µ—Ç—Å—è —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å <pre>...</pre>.
    """
    text = text.strip()
    opens = text.count("<pre>")
    closes = text.count("</pre>")
    if opens > closes:
        diff = opens - closes
        text += "</pre>" * diff
    elif closes > opens:
        diff = closes - opens
        for _ in range(diff):
            idx = text.rfind("</pre>")
            if idx >= 0:
                text = text[:idx] + text[idx + 6:]
    return text


async def send_long_html(bot: Bot, chat_id: int, text: str, chunk_size=4000):
    """
    –î—Ä–æ–±–∏–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º parse_mode=HTML.
    """
    text = fix_html_tags(text.strip())
    chunks = textwrap.wrap(text, width=chunk_size, replace_whitespace=False)
    for chunk in chunks:
        chunk = fix_html_tags(chunk)
        await bot.send_message(chat_id, chunk, parse_mode="HTML")
